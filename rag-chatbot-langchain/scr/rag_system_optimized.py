import os
import json
import logging
import requests
from datetime import datetime
from typing import Dict, Optional
from dotenv import load_dotenv

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Carrega variÃ¡veis de ambiente
load_dotenv()


class RAGChatbotFinal:
    """Sistema RAG Completo com Google Gemini - VersÃ£o Final."""

    def __init__(self):
        """Inicializa o sistema RAG."""
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.config = self._get_config()
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
        self.session_stats = {
            "queries_made": 0,
            "total_tokens": 0,
            "start_time": datetime.now()
        }

        print("ğŸš€ Sistema RAG inicializado com sucesso!")

    def _get_config(self) -> Dict:
        """ConfiguraÃ§Ã£o otimizada (sem dependÃªncia de arquivo externo)."""
        return {
            "llm_model": "gemini-1.5-flash",
            "temperature": 0.7,
            "max_results": 3,
            "cache_size": 100,
            "rate_limit_requests": 10,
            "rate_limit_window": 60,
            "min_query_length": 3,
            "max_query_length": 1000,
            "timeout": 30
        }

    def validate_setup(self) -> bool:
        """Valida se tudo estÃ¡ configurado corretamente."""
        if not self.api_key:
            print("âŒ API Key nÃ£o encontrada!")
            print("ğŸ“ Configure no arquivo .env: GOOGLE_API_KEY=sua_chave")
            print("ğŸ”— Obtenha sua chave em: https://aistudio.google.com/")
            return False

        print("âœ… API Key configurada!")
        print("âœ… Sistema pronto para uso!")
        return True

    def _validate_query(self, query: str) -> tuple[bool, str]:
        """Valida a query de entrada."""
        if not query or not query.strip():
            return False, "Pergunta nÃ£o pode estar vazia"

        query = query.strip()

        if len(query) < self.config["min_query_length"]:
            return False, f"Pergunta muito curta (mÃ­nimo {self.config['min_query_length']} caracteres)"

        if len(query) > self.config["max_query_length"]:
            return False, f"Pergunta muito longa (mÃ¡ximo {self.config['max_query_length']} caracteres)"

        return True, query

    def _create_optimized_prompt(self, query: str) -> str:
        """Cria um prompt otimizado para o Gemini."""
        return f"""VocÃª Ã© um assistente inteligente e prestativo. Responda Ã  seguinte pergunta de forma clara, precisa e educativa.

INSTRUÃ‡Ã•ES:
- Seja direto e objetivo
- Use exemplos prÃ¡ticos quando apropriado
- Se nÃ£o souber algo, seja honesto sobre isso
- Mantenha um tom profissional e amigÃ¡vel
- Estruture sua resposta de forma clara

PERGUNTA: {query}

RESPOSTA:"""

    def call_gemini_api(self, query: str) -> Dict:
        """Chama a API do Google Gemini."""
        try:
            # Valida a query
            is_valid, processed_query = self._validate_query(query)
            if not is_valid:
                return self._create_error_response(query, processed_query)

            # Prepara headers e payload
            headers = {"Content-Type": "application/json"}

            prompt = self._create_optimized_prompt(processed_query)

            payload = {
                "contents": [
                    {
                        "parts": [{"text": prompt}]
                    }
                ],
                "generationConfig": {
                    "temperature": self.config["temperature"],
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": 2048,
                    "stopSequences": []
                },
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                    }
                ]
            }

            url = f"{self.base_url}?key={self.api_key}"

            print("ğŸ”„ Consultando Google Gemini...")

            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.config["timeout"]
            )

            if response.status_code == 200:
                result = response.json()
                return self._process_gemini_response(processed_query, result)
            else:
                error_msg = f"Erro HTTP {response.status_code}: {response.text[:200]}"
                logger.error(error_msg)
                return self._create_fallback_response(processed_query, error_msg)

        except requests.exceptions.Timeout:
            error_msg = "Timeout na consulta ao Gemini"
            logger.error(error_msg)
            return self._create_fallback_response(query, error_msg)

        except requests.exceptions.ConnectionError:
            error_msg = "Erro de conexÃ£o. Verifique sua internet"
            logger.error(error_msg)
            return self._create_fallback_response(query, error_msg)

        except Exception as e:
            error_msg = f"Erro inesperado: {str(e)}"
            logger.error(error_msg)
            return self._create_fallback_response(query, error_msg)

    def _process_gemini_response(self, query: str, result: Dict) -> Dict:
        """Processa a resposta do Gemini."""
        try:
            if 'candidates' in result and len(result['candidates']) > 0:
                candidate = result['candidates'][0]
                content = candidate['content']['parts'][0]['text']

                # Atualiza estatÃ­sticas
                self.session_stats["queries_made"] += 1

                return {
                    "query": query,
                    "result": content.strip(),
                    "model": self.config["llm_model"],
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "source": "Google Gemini",
                    "real_api": True,
                    "session_query_count": self.session_stats["queries_made"]
                }
            else:
                raise Exception("Resposta vazia ou invÃ¡lida do Gemini")

        except Exception as e:
            logger.error(f"Erro ao processar resposta: {e}")
            return self._create_fallback_response(query, str(e))

    def _create_error_response(self, query: str, error: str) -> Dict:
        """Cria resposta de erro."""
        return {
            "query": query,
            "result": f"Erro: {error}",
            "model": "sistema",
            "timestamp": datetime.now().isoformat(),
            "status": "error",
            "source": "Sistema Local",
            "real_api": False
        }

    def _create_fallback_response(self, query: str, error: str) -> Dict:
        """Cria resposta de fallback em caso de erro."""
        fallback_text = f"""Desculpe, nÃ£o foi possÃ­vel processar sua pergunta no momento devido a: {error}

Como alternativa, aqui estÃ¡ uma resposta bÃ¡sica sobre sua pergunta: "{query}"

Esta Ã© uma resposta simulada. Para obter respostas mais precisas, verifique sua conexÃ£o com a internet e tente novamente."""

        return {
            "query": query,
            "result": fallback_text,
            "model": "fallback",
            "timestamp": datetime.now().isoformat(),
            "status": "fallback",
            "source": "Sistema Local",
            "real_api": False,
            "error": error
        }

    def display_response(self, response: Dict):
        """Exibe resposta formatada."""
        print(f"\nğŸ¤– Pergunta: {response['query']}")
        print(f"ğŸ“ Resposta: {response['result']}")
        print(f"ğŸ”§ Modelo: {response['model']}")

        if response.get('real_api'):
            print("âœ… Resposta real do Google Gemini!")
        elif response['status'] == 'error':
            print("âŒ Erro na consulta!")
        else:
            print("âš ï¸  Resposta simulada (modo fallback)")

        print(f"ğŸ“Š Status: {response['status']}")
        print(f"â° Timestamp: {response['timestamp']}")

        if response.get('session_query_count'):
            print(f"ğŸ“ˆ Consultas na sessÃ£o: {response['session_query_count']}")

        print("-" * 80)

    def interactive_chat(self):
        """Chat interativo avanÃ§ado."""
        print("=" * 80)
        print("ğŸ¤– CHAT RAG COM GOOGLE GEMINI - VERSÃƒO AVANÃ‡ADA")
        print("=" * 80)
        print("Comandos especiais:")
        print("  â€¢ 'sair' ou 'quit' - Encerra o chat")
        print("  â€¢ 'ajuda' - Mostra comandos disponÃ­veis")
        print("  â€¢ 'stats' - Mostra estatÃ­sticas da sessÃ£o")
        print("  â€¢ 'limpar' - Limpa a tela")
        print("  â€¢ 'teste' - Testa conexÃ£o com Gemini")
        print("=" * 80)
        print("ğŸ’¡ Dica: Seja especÃ­fico em suas perguntas para obter melhores respostas!")
        print("=" * 80)

        while True:
            try:
                query = input("\nğŸ’¬ Sua pergunta: ").strip()

                if not query:
                    print("âš ï¸  Digite uma pergunta ou comando.")
                    continue

                # Comandos especiais
                if query.lower() in ['sair', 'quit', 'exit']:
                    self._show_session_summary()
                    print("ğŸ‘‹ Obrigado por usar o sistema RAG! AtÃ© logo!")
                    break

                elif query.lower() == 'ajuda':
                    self._show_help()
                    continue

                elif query.lower() == 'stats':
                    self._show_statistics()
                    continue

                elif query.lower() == 'limpar':
                    os.system('cls' if os.name == 'nt' else 'clear')
                    print("ğŸ§¹ Tela limpa!")
                    continue

                elif query.lower() == 'teste':
                    print("ğŸ”„ Testando conexÃ£o com Google Gemini...")
                    test_response = self.call_gemini_api("OlÃ¡! VocÃª estÃ¡ funcionando?")
                    self.display_response(test_response)
                    continue

                # Processa pergunta normal
                response = self.call_gemini_api(query)
                self.display_response(response)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Chat interrompido pelo usuÃ¡rio. AtÃ© logo!")
                break
            except Exception as e:
                logger.error(f"Erro no chat: {e}")
                print(f"âŒ Erro inesperado: {e}")

    def _show_help(self):
        """Mostra ajuda detalhada."""
        print("\nğŸ“– AJUDA - COMANDOS DISPONÃVEIS")
        print("-" * 50)
        print("ğŸ—£ï¸  PERGUNTAS NORMAIS:")
        print("   â€¢ Digite qualquer pergunta e pressione Enter")
        print("   â€¢ Exemplos: 'O que Ã© IA?', 'Como funciona Python?'")
        print("\nâš™ï¸  COMANDOS ESPECIAIS:")
        print("   â€¢ 'sair' / 'quit' - Encerra o programa")
        print("   â€¢ 'ajuda' - Mostra esta ajuda")
        print("   â€¢ 'stats' - EstatÃ­sticas da sessÃ£o")
        print("   â€¢ 'limpar' - Limpa a tela")
        print("   â€¢ 'teste' - Testa conexÃ£o com Gemini")
        print("\nğŸ’¡ DICAS:")
        print("   â€¢ Seja especÃ­fico em suas perguntas")
        print("   â€¢ Use portuguÃªs ou inglÃªs")
        print("   â€¢ Evite perguntas muito longas")

    def _show_statistics(self):
        """Mostra estatÃ­sticas da sessÃ£o."""
        duration = datetime.now() - self.session_stats["start_time"]

        print(f"\nğŸ“Š ESTATÃSTICAS DA SESSÃƒO")
        print("-" * 40)
        print(f"ğŸ•’ Tempo de sessÃ£o: {duration}")
        print(f"ğŸ“ˆ Consultas realizadas: {self.session_stats['queries_made']}")
        print(f"ğŸ¤– Modelo em uso: {self.config['llm_model']}")
        print(f"ğŸŒ¡ï¸  Temperature: {self.config['temperature']}")
        print(f"â±ï¸  Timeout: {self.config['timeout']}s")

    def _show_session_summary(self):
        """Mostra resumo da sessÃ£o."""
        duration = datetime.now() - self.session_stats["start_time"]

        print("\n" + "=" * 60)
        print("ğŸ“Š RESUMO DA SESSÃƒO")
        print("=" * 60)
        print(f"â° DuraÃ§Ã£o total: {duration}")
        print(f"ğŸ”¢ Total de consultas: {self.session_stats['queries_made']}")
        print(f"ğŸ¤– Modelo utilizado: {self.config['llm_model']}")
        print("=" * 60)

    def quick_query(self, question: str) -> Dict:
        """Consulta rÃ¡pida para uso programÃ¡tico."""
        return self.call_gemini_api(question)


def main():
    """FunÃ§Ã£o principal do programa."""
    print("ğŸš€ Sistema RAG com Google Gemini - VersÃ£o Final")
    print("=" * 55)

    try:
        # Inicializa o sistema
        rag_system = RAGChatbotFinal()

        # Valida configuraÃ§Ã£o
        if not rag_system.validate_setup():
            print("\nâŒ Sistema nÃ£o pode continuar sem API Key vÃ¡lida.")
            input("Pressione Enter para sair...")
            return

        print(f"\nğŸ¯ Escolha como usar o sistema:")
        print("1. ğŸ’¬ Chat interativo completo")
        print("2. â“ Pergunta Ãºnica")
        print("3. ğŸ§ª Teste de conexÃ£o")
        print("4. ğŸ“Š Apenas informaÃ§Ãµes do sistema")

        choice = input("\nSua escolha (1-4): ").strip()

        if choice == "1":
            rag_system.interactive_chat()

        elif choice == "2":
            question = input("\nğŸ’­ Digite sua pergunta: ").strip()
            if question:
                print("\nğŸ”„ Processando pergunta...")
                response = rag_system.call_gemini_api(question)
                rag_system.display_response(response)
            else:
                print("âŒ Pergunta nÃ£o pode estar vazia!")

        elif choice == "3":
            print("\nğŸ”¬ Executando teste de conexÃ£o...")
            test_response = rag_system.call_gemini_api("Teste de conexÃ£o: vocÃª estÃ¡ funcionando?")
            rag_system.display_response(test_response)

        elif choice == "4":
            rag_system._show_statistics()

        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida! Iniciando chat interativo...")
            rag_system.interactive_chat()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Programa interrompido. AtÃ© logo!")
    except Exception as e:
        logger.error(f"Erro crÃ­tico: {e}")
        print(f"âŒ Erro crÃ­tico: {e}")

    print("\nğŸ‰ Obrigado por usar o Sistema RAG!")
    input("Pressione Enter para sair...")


if __name__ == "__main__":
    main()